{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc5d89a-6a94-4a54-8818-791a29a1bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sales_models as sm\n",
    "X = sm.PriorsPlayML(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b86c88-7fbb-4a72-bc98-ec82d607653d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this Play-ML notebook we are going to explore the concept of **uncertainty quantification**. We will explore UQ in the context of recommending a price to set for a product. One very important reason that decisions are difficult to take is because of uncertainty: how can we be sure that *this* is the right thing to do and not *that*? And, just as importantly, what's the risk of getting things wrong? \n",
    "\n",
    "So let's illustrate uncertainty quantification in the context of price recommendations. Suppose we own a website that sells exactly one kind of product&mdash;a deluxe mud kitchen, say&mdash;and that any customer will only buy at most one. Indeed, you really only need one mud kitchen, *deluxe* or otherwise. In this notebook we will think about a simple model for how sales of our mud kitchen depend on the price we set, and use that model to try to recommend a good price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6b64d-3e0f-4a38-8fa2-84a420e68010",
   "metadata": {},
   "source": [
    "## A toy model\n",
    "\n",
    "<img align=\"right\" width=\"350\" src=\"./mudkitchen.jpg\">\n",
    "\n",
    "The model is described by three parameters. There is an **arrival rate**, at which potential customers stumble across our site. For simplicity we assume that the price of our mud kitchen is not marketed nor scraped from our website: the arrival rate is fixed at a number of customers per day and doesn't vary with price.\n",
    "\n",
    "Where price does enter the picture is in the conversion rate: the probability that any one customer on our website will make a (single) purchase. This is assumed to be very much price dependent: when the price is low, this probability very high, but as the price increases the probability of a purchase will decrease to zero. We model this behaviour with a logistic curve, which gives two degrees of freedom to set. The first of these is the **reference price**, which can be defined as the price at which the probability of a customer making a purchase is 50%. The second parameter characterising the price response is the **reference elasticity**. This parameter, denoted by $E$, is interpreted as a measure of price sensitivity: if the price is at a 1% premium on the reference price, then demand will increase by $E$%. Of course, we would expect demand to *decrease*, which is why elasticity is usually negative. \n",
    "\n",
    "These three numbers are enough to fully characterise our model, the outputs of which are shown below. You can play around with the values of these parameters and observe what happens to the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a4d851-adfd-4747-a0ab-2bf7eb0e2e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd26083c4ab461d8387549a2e07b627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(IntSlider(value=1000, continuous_update=False, description='Arriv…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_nominal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3ee357-d789-4186-a294-ea29b91c5be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13750.0, 13060.87980251604)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = X.the_true_model\n",
    "L1, L2 = (60, 70)\n",
    "C = 35\n",
    "P1 = m.expected_sales(L1) * (L1 - C)\n",
    "P2 = m.expected_sales(L2) * (L2 - C)\n",
    "P1, P2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4a9cf-db70-499b-a0d8-bc1ddde5727a",
   "metadata": {},
   "source": [
    "The first output, in the top left, is the **Purchase probability** curve in response to price. This follows the rule of decreasing from almost 100% to almost zero as price increases beyond the reference price, but the precise shape of this curve can be influenced by both the reference price and reference elasticity.\n",
    "\n",
    "The top right curve is the **elasticity of demand** at different prices. When above -1, people are less sensitive to price changes, in the sense that price increases will decrease demand relatively gently. At prices where elasticity falls below -1, the model suggests that customers are more sensitive to prices, and price decreases will generate a comparatively large increase in demand. The balance is found at elasticity of -1.\n",
    "\n",
    "The bottom left curve represents the daily **sales** at each price, where a ribbon around the green curve can be observed. This ribbon reflects the fact that  our model incorporates randomness. The website has a random number of arrivals at a given rate, each of whom makes a purchase at random with a given probability. These two steps convert to a random number of sales. The green curve shows the long-run average of this distribution, while the ribbon shows the range of values of realised sales on any one day.\n",
    "\n",
    "The bottom right curve translates the random number of sales into a random daily **revenue**. Again, there is a ribbon around the red curve: the revenue in any one day at a given price is likely to fall somewhere within the ribbon. However, in the long run, the daily revenue at any price will take the corresponding value of the red curve. The key observation to make here is that a price can be selected at which the expected revenue is maximised. This price is exactly the price at which elasticity is equal to -1, as identified in the top right output. This means that if we know the parameters of the model, we can turn that knowledge into a decision. Our price recommendation strategy will be to select the price where elasticity is -1, which will maximise the expected revenue. The revenue-maximising price is recorded in the top right of the figure above, where you can observe how the optimal price varies with the model parameters. \n",
    "\n",
    "By exploring a few different model parameters, an important observation can be made here: the arrival rate has no impact on the revenue-maximising price. However, the *value* of ensuring the correct decision does change: the revenue curve scales with the arrival rate, and so the benefits of getting the decision correct will vary with arrival rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd82232-1302-4b45-8768-478e7359f2fc",
   "metadata": {},
   "source": [
    "# Learning model parameters from data\n",
    "\n",
    "In the exploration above, the model parameters could all be set by manipulating the sliders. However, we want our model to be reflective of reality. While we can set the price, we do *not* set the arrival rate, reference price and reference elasticity. These parameters are all quantities that are characteristic to our market, and aren't known *a priori*. We've observed how the optimal decision is dependent on the parameter values, and so we need to learn them from data.\n",
    "\n",
    "Here, we've generated some synthetic sales data directly from our model, with the parameters set to some (hidden) values. We can play around with the prices to produce new data sets, but all of the data is produced based on these same, fixed, hidden parameter values. The game here is to ask whether we can learn the parameters that generated the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc431a98-c91b-4836-84d3-9f9488d62248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e446fefcbc490fbe64f4f4695a30d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntSlider(value=50, continuous_update=False, description='Mon: ', layout=Layout(…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67d477-be78-43a7-b63b-281d66241b5d",
   "metadata": {},
   "source": [
    "The figures below show the outcome of a classical approach to learning the parameters of this model. The three parameters are chosen to maximise the *likelihood* of the synthetic data generated above. That is, we find values for the three parameters such that the probability of the observed data under the resulting model is highest. This is known as the **maximum likelihood estimate**, or MLE. We can learn sequentially, refining our estimate over each of the seven days, based only on the data observed up to that point. So, the learned parameter values on Monday are based only on Monday's data; on Tuesday, they are based on both Monday and Tuesday's data; and so on, until the learned parameter values on Sunday are based on the entire week's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d01cae9-f7e7-4bad-a7da-40f2ace2bd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91943ef4fef7465ca9707ee2ef897e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 1500x480 with 3 Axes>', '…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_MLEs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c77ef-8c6c-4b4d-b5f1-06a20ea5387d",
   "metadata": {
    "tags": []
   },
   "source": [
    "On each day, we see how the three parameter values are estimated based on the supplied data. The left-hand plot compares the three parameter values produced by each day's MLE against the true (hidden) parameter values that actually generated the synthetic data. Depending on the data supplied, these estimates can be relatively close or suspiciously far away from the true values. The middle plot shows how the estimated parameter values do tend to produce marginally better fits to the data than the true parameter values. Where they do not, this indicates a failure to optimise correctly, and no optimisation algorithm is failsafe (even in this simple example). Finally, the right-hand plot shows the impact of the estimated parameter values on the corresponding price recommendations: depending on the synthetic data supplied, these are sometimes near the true optimal price, but sometimes they are concerningly far away.\n",
    "\n",
    "Note that these figures are only available to us because we know the true parameter values that generated the data. It is worth reiterating that these are not known in the real world applications of this method!\n",
    "\n",
    "The key observation here is that we are not quantifying *uncertainty*. The MLE parameter values are produced by an optimisation, and result in a price recommendation. Both the parameters and the resulting price are point estimates only. The middle plot makes it clear that by optimising likelihood, we are discarding different parameter values that also give a very good fit to data&mdash;not least, the *true* parameter values! We would like to be more open minded about all of those parameter values that also fit the data very well, since they might be the correct ones.\n",
    "\n",
    "What's more, the MLE parameter values are not stable; as more data are supplied, the estimates are updated, sometimes quite significantly. One might assume that more data will make our estimate more reliable, but there is not much indication of *how* much more reliable our decision will be on Sunday than it was on Tuesday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc259d71-f8be-4965-b015-ce23ccb12df2",
   "metadata": {},
   "source": [
    "# Uncertainty\n",
    "\n",
    "We will now explore in more detail what we mean by uncertainty. Recall that in the top figure above, the three parameter values are set by the slider. These parameter values directly generated an optimal price, in terms of maximising the long-term expected daily revenue.\n",
    "\n",
    "Now suppose we only roughly know the parameter values, in the sense that we know the arrival rate is near 1000, that the reference price is near £50, and that the reference elasticity is near -1.5. How near? We will quantify this by a scaling parameter that represents the degree of uncertainty in these values. We put a *distribution* onto parameter space, that represents our uncertainty. Because of this distribution, we can make statements that *quantify* our uncertainty. For example, suppose we set the top slider to 50. Then we are specifying that we are 70% sure that the arrival rate is between 950 and 1050."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bb0b85-d021-47bc-9161-3d68f1517986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934f659010794e3b92759504aced2876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(IntSlider(value=50, continuous_update=False, description='Arrival rate uncertain…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_prior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524dcf-2868-430c-aca4-15cc996526ff",
   "metadata": {},
   "source": [
    "If we know the parameters, then we can make an optimal decision. If the parameters are uncertain, then the optimal decision is uncertain. Having quantified the uncertainty in the parameters, this means we can quantify the uncertainty in the resulting decision. The purple distribution in the plot above shows how the distribution in the parameters is mapped to a distribution in the optimal decision, so that it is possible to quantify the uncertainty in our decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d42218-62e4-4f87-8976-9c66ea6383cb",
   "metadata": {},
   "source": [
    "Below, we show how this uncertainty propagates further, into all aspects of the model: as the parameters vary within their distribution, the price-dependent values of purchase probability, elasticity, expected sales and expected revenue all reflect this uncertainty. The ribbons on each curve reflect our uncertainty: darker ribbons represent the middle 50% of the uncertainty distribution and lighter ribbons the middle 90% of the uncertainty distribution. The solid curves here represent the medians of each uncertainty distribution. Depending on how uncertain we are in our parameter values, the model outputs can be very vague indeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a357450-9a07-474d-b79c-21b964de799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce85286c2824ae3a7906507cb4b6d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='Prior predictive distributions', options=('Purchase probability', 'El…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_prior_predictive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e74e83-74e3-466c-8667-474a5cd9abfe",
   "metadata": {},
   "source": [
    "# Taming uncertainty with data\n",
    "\n",
    "Before introducing uncertainty, we learned some maximum likelihood estimates from data. These produced point values for each of the parameters, based on the results of optimisation. Now, we are going to work with distributions rather than single points, because a different question is being answered: how is uncertainty changing in the context of data?\n",
    "\n",
    "## Thomas Bayes\n",
    "\n",
    "The approach to this question is named after an 18th century English nonconformist minister, The Reverend Thomas Bayes, a mathematically inclined Fellow of the Royal Society. A paper of his, posthumously read to the Royal Society, is the source of Bayes' theorem. This is the mathematical basis of **Bayesian statistics**, enabling the combination of uncertainty, represented as a *prior distribution*, with data, to produce a *posterior distribution* to represent our new uncertainty, informed by data. By quirk of location, Bayes and Datasparq have an interesting affinity: Bayes spent some time preaching at a Presbyterian chapel in Leather Lane, and he is now buried in Bunhill Fields&mdash;both locations within a few minutes' walk of the Datasparq offices in Clerkenwell.\n",
    "\n",
    "## Sequential Bayesian learning\n",
    "\n",
    "Recall the data used earlier to acquire the MLEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f60c36b-8f29-4c2d-a0e6-6b877ad3b77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d9cf320fa0421b8827eb6881665bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 2 Axes>', 'i…"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ccf7b-6938-4299-bfe5-501df1e5848c",
   "metadata": {},
   "source": [
    "As with the MLE approach, we can proceed day by day and sequentially update our uncertainty in the context of each day's data. Now, though, the output from each day is a distribution rather than a point estimate. In the figure below, with each press of the [LEARN!] button, each day's observation is integrated with the current state of the uncertainty to produce a new, posterior, distribution for the parameters. As a result, the uncertainty can be observed evolving day by day, as more information comes in. The vertical dotted lines represent the parameter values used to generate the synthetic data. The evolving uncertainty can be evaluated against this ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49531ba-60df-456b-b70f-2267a666f7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483085d5fecf4aed8c28d4db499ae352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(IntSlider(value=50, continuous_update=False, description='Arrival…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff65341-6569-4c26-bd2f-58aa8b73537c",
   "metadata": {},
   "source": [
    "Having updated the uncertainty in the light of data, the purple distribution above represents the corresponding uncertainty in our price recommendation. The vertical dotted line is the ground-truth optimal price corresponding to the data-generating parameters. A new pricing strategy can be considered based on this distribution. For example, we might pick the price where this distribution peaks, as this is what the model (in combination with the prior and the data) judges as the most likely to be optimal. Alternatively, we might pick a price at random from this distribution, to make use of any of the possible prices according to how likely they are to be optimal. Either way, this distribution makes clear how confident we can feel in this price recommendation.\n",
    "\n",
    "Below, we once again show how our uncertainty, now conditioned on data, propagates into the model outputs. As with the plots above, the black dotted lines now refer to the ground truth model, for the parameters that generated the synthetic data. Having learnt posterior distributions from combining our prior uncertainty with data, we can observe how our uncertain estimates of the model outputs accord with this reality. Of course, this reality is not knowable in real-world contexts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427aaa01-6bf0-4bf9-ac3a-27e3f0940de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1138e2eae94ee79c53f1ed81a537ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='Posterior predictive distributions', options=('Purchase probability',…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_posterior_predictive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9ed7c-10fb-4fa3-90e5-d4a1dc3e34e3",
   "metadata": {},
   "source": [
    "## Pricing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd407a-d105-4bef-9685-a1ded0d75451",
   "metadata": {},
   "source": [
    "After incorporating the full week's data with the prior, there are at least three possible strategies to follow. The first is to take the price recommendation corresponding to the MLE parameter estimate. That is, to ignore the prior and any uncertainty. The second is to take the price corresponding to the peak of the uncertain distribution for the optimal price. The third is to randomly choose a different price every day from the uncertain distribution of the optimal price.\n",
    "\n",
    "Below, we show the consequences of using these price recommendation strategies to set a price for the true model, and compare the resulting revenue against the optimal price. By exploring different data sets and priors, we observe how the price recommendations can be improved or damaged by incorporating data with prior uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1135bd2-8213-4024-81db-abf00d06b1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abddfb0a5c484cedb917286f3f03a691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 wi…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.visualise_price_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff09b7-4453-45f2-b74b-e8d976a03fee",
   "metadata": {},
   "source": [
    "# Reflections\n",
    "\n",
    "## The toy model\n",
    "\n",
    "A mud kitchen is a toy model which doesn't contain all the detail of its real world counterpart. So too is the model explored in this notebook.\n",
    "\n",
    "In particular, this model has been deliberately designed to be difficult to identify from the supplied data. Suppose we see 1000 sales. Is that 5000 arrivals at our website with a purchase probability of 20%? Or is it 1250 arrivals at 80%? By only supplying sales data, we cannot unpick those two numbers. This concept is known as *parameter unidentifiability*. A similar phenomenon happens when our synthetic data is generated using only one price: how is price sensitivity going to be estimated without observing sales at different prices?\n",
    "\n",
    "When only considering the MLE approach, this unidentifiability is not obvious from the outputs. An estimate is produced regardless, with no indication of how good it might be. However, we might look again at the posterior distribution of the arrival rate for the uncertain, Bayesian approach. This distribution is typically more spread out than the prior, in the sense of being less peaked. The interpretation of that would be that the data, in combination with the prior, has made us less certain than we were at the start. This is a good thing to know! It is an important indicator that our data is insufficient for identifying the model. \n",
    "\n",
    "## Priors and subjectivity in Bayesian analysis\n",
    "\n",
    "Another important distinction between a Bayesian approach and the MLE approach is the use of *priors*: the quantification of uncertainty upfront, before incorporating any data. This is a subjective judgement call. Used well, this is a strength of the Bayesian approach; used badly, there is the potential to cause significant damage. A key problem is overconfidence. Nothing stops us overasserting ourselves at the start of the analysis, and the Bayesian approach does not immediately identify overconfidence. If we push all of the sliders representing uncertainty to the left, we are saying that we are very confident about the parameter values&mdash;even though we are wrong. This confidence tends to override the contradictory information provided by the small amount of data used in this toy example.\n",
    "\n",
    "The design of priors is extremely important to powerful uncertainty quantification methods. Well-designed, open-minded but accurate priors can overcome the parameter unidentifiability problem by incorporating expertise and tacit knowledge where data is lacking. However, when designed badly, they can lead to severely suboptimal, overconfident recommendations. Prior design is one aspect of Bayesian analysis where close collaboration between data scientists and domain experts is absolutely vital.\n",
    "\n",
    "## The super-deluxe toy model\n",
    "\n",
    "There is some passivity in going from our first figure, where the parameters were set, to the subsequent discussion of learning 'true' parameters. The real world is somewhere in-between. We might well have some real parameters, but other decisions we take can affect these. External marketing, website copy, an improved product (a *super*-deluxe mud kitchen!) can all affect these parameters. However, measuring these effects can only be done in the context of quantified uncertainty. Different customer types (parents vs grandparents, for example) may well have quite different arrival rates at our website, reference prices and price sensitivities, but to what extent? What data do we need for these to be identifiable? Are they different enough that we can detect them in the context of uncertainty? And of course time plays an important role. How does the market for a fun outdoor toy change near Christmas? Uncertainty pervades all of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c483967-75cf-4377-8dbd-3617b48a9f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
